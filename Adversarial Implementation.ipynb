{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation,Flatten\n",
    "from keras import regularizers\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn import metrics\n",
    "from keras.models import model_from_json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class definitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data():\n",
    "    def __init__(self,data):\n",
    "        self.data=data\n",
    "        self.dataSmote=pd.DataFrame()\n",
    "        \n",
    "    def sample(self,start,end):\n",
    "        size=len(self.data)\n",
    "        return self.data[int(size*start):int(size*end)]\n",
    "    \n",
    "    def getData():\n",
    "        return self.dataSmote\n",
    "\n",
    "    def sample_smote(self,start,end):\n",
    "        data=self.sample(start,end)\n",
    "        self.dataSmote=np.array(data.drop(\"Class\",axis=1))\n",
    "        y=np.array(data[['Class']])\n",
    "        smt=SMOTE()\n",
    "        self.dataSmote,y=smt.fit_sample(self.dataSmote,y)\n",
    "        self.dataSmote=pd.DataFrame(self.dataSmote)\n",
    "        y=pd.DataFrame(y)\n",
    "        self.dataSmote['Class']=y\n",
    "        self.dataSmote=self.dataSmote.sample(frac=1)\n",
    "        self.dataSmote.columns=list(data.columns)\n",
    "        return self.dataSmote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aggregator():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.wB1=0.55\n",
    "        self.wB2=0.35\n",
    "        self.wB3=0.10\n",
    "    \n",
    "    def aggregate(self,delta,B1,B2,B3):\n",
    "        delta=np.array(delta)\n",
    "        temp=(self.wB1*np.array(B1) + self.wB2*np.array(B2) + self.wB3*np.array(B3))\n",
    "        temp-=delta\n",
    "        delta+=temp\n",
    "        \n",
    "        return delta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.input_shape=(30,)\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(32, activation='relu',input_shape=self.input_shape))\n",
    "        self.model.add(Dense(16, activation='relu'))\n",
    "        self.model.add(Dense(8, activation='relu'))\n",
    "        self.model.add(Dense(1, activation='sigmoid'))\n",
    "        self.model.compile(optimizer='adam',   #rmsprop\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    def saveModel(self):\n",
    "        model_json = self.model.to_json()\n",
    "        with open(\"model.json\", \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        # serialize weights to HDF5\n",
    "        self.model.save_weights(\"model.h5\")\n",
    "        print(\"Saved model to disk\")\n",
    "        \n",
    "    def loadModel(self):\n",
    "        json_file = open('model.json', 'r')\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        loaded_model = model_from_json(loaded_model_json)\n",
    "        # load weights into new model\n",
    "        loaded_model.load_weights(\"model.h5\")\n",
    "        print(\"Loaded model from disk\")\n",
    "        loaded_model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "        return loaded_model    \n",
    "    \n",
    "    def getModel(self):\n",
    "        return self.model\n",
    "\n",
    "\n",
    "    def run(self,X,Y,validation_split=0,load=True):\n",
    "        if(load):\n",
    "            self.model=self.loadModel()\n",
    "        self.model.fit(X,Y,epochs=5,validation_split=validation_split, verbose=1)\n",
    "        \n",
    "    def evaluate(self,X,Y):\n",
    "        return self.model.evaluate(X,Y)[1]*100\n",
    "    \n",
    "    def loss(self,X,Y):\n",
    "        return self.model.evaluate(X,Y)[0]\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return self.model.predict(X)\n",
    "        \n",
    "    def getLayers(self):\n",
    "        return self.model.layers\n",
    "    \n",
    "    def getWeights(self):\n",
    "        return self.model.get_weights()\n",
    "    \n",
    "    def setWeights(self,weight):\n",
    "        self.model.set_weights(weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bank(Model):\n",
    "    \n",
    "    def __init__(self,data,split_size=0):\n",
    "        super().__init__()\n",
    "        self.data=data\n",
    "        self.split(split_size)\n",
    "    \n",
    "    def setData(self,data,split_size=0):\n",
    "        self.data=data\n",
    "        self.split(split_size)\n",
    "        \n",
    "    def getData(self):\n",
    "        return self.data\n",
    "    \n",
    "    def split(self,split_size):\n",
    "        X=self.data.copy()\n",
    "        X.drop(['Class'],axis=1,inplace=True)\n",
    "        Y=self.data[['Class']]\n",
    "\n",
    "        if split_size == 0:\n",
    "            self.X_train, self.X_test, self.Y_train, self.Y_test = X,X,Y,Y\n",
    "        else:\n",
    "            self.X_train, self.X_test, self.Y_train, self.Y_test = train_test_split(X, Y, test_size=split_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n\n         V8        V9  ...       V21       V22       V23       V24       V25  \\\n0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n\n        V26       V27       V28  Amount  Class  \n0 -0.189115  0.133558 -0.021053  149.62      0  \n1  0.125895 -0.008983  0.014724    2.69      0  \n2 -0.139097 -0.055353 -0.059752  378.66      0  \n3 -0.221929  0.062723  0.061458  123.50      0  \n4  0.502292  0.219422  0.215153   69.99      0  \n\n[5 rows x 31 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>-1.359807</td>\n      <td>-0.072781</td>\n      <td>2.536347</td>\n      <td>1.378155</td>\n      <td>-0.338321</td>\n      <td>0.462388</td>\n      <td>0.239599</td>\n      <td>0.098698</td>\n      <td>0.363787</td>\n      <td>...</td>\n      <td>-0.018307</td>\n      <td>0.277838</td>\n      <td>-0.110474</td>\n      <td>0.066928</td>\n      <td>0.128539</td>\n      <td>-0.189115</td>\n      <td>0.133558</td>\n      <td>-0.021053</td>\n      <td>149.62</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>1.191857</td>\n      <td>0.266151</td>\n      <td>0.166480</td>\n      <td>0.448154</td>\n      <td>0.060018</td>\n      <td>-0.082361</td>\n      <td>-0.078803</td>\n      <td>0.085102</td>\n      <td>-0.255425</td>\n      <td>...</td>\n      <td>-0.225775</td>\n      <td>-0.638672</td>\n      <td>0.101288</td>\n      <td>-0.339846</td>\n      <td>0.167170</td>\n      <td>0.125895</td>\n      <td>-0.008983</td>\n      <td>0.014724</td>\n      <td>2.69</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>-1.358354</td>\n      <td>-1.340163</td>\n      <td>1.773209</td>\n      <td>0.379780</td>\n      <td>-0.503198</td>\n      <td>1.800499</td>\n      <td>0.791461</td>\n      <td>0.247676</td>\n      <td>-1.514654</td>\n      <td>...</td>\n      <td>0.247998</td>\n      <td>0.771679</td>\n      <td>0.909412</td>\n      <td>-0.689281</td>\n      <td>-0.327642</td>\n      <td>-0.139097</td>\n      <td>-0.055353</td>\n      <td>-0.059752</td>\n      <td>378.66</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>-0.966272</td>\n      <td>-0.185226</td>\n      <td>1.792993</td>\n      <td>-0.863291</td>\n      <td>-0.010309</td>\n      <td>1.247203</td>\n      <td>0.237609</td>\n      <td>0.377436</td>\n      <td>-1.387024</td>\n      <td>...</td>\n      <td>-0.108300</td>\n      <td>0.005274</td>\n      <td>-0.190321</td>\n      <td>-1.175575</td>\n      <td>0.647376</td>\n      <td>-0.221929</td>\n      <td>0.062723</td>\n      <td>0.061458</td>\n      <td>123.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0</td>\n      <td>-1.158233</td>\n      <td>0.877737</td>\n      <td>1.548718</td>\n      <td>0.403034</td>\n      <td>-0.407193</td>\n      <td>0.095921</td>\n      <td>0.592941</td>\n      <td>-0.270533</td>\n      <td>0.817739</td>\n      <td>...</td>\n      <td>-0.009431</td>\n      <td>0.798278</td>\n      <td>-0.137458</td>\n      <td>0.141267</td>\n      <td>-0.206010</td>\n      <td>0.502292</td>\n      <td>0.219422</td>\n      <td>0.215153</td>\n      <td>69.99</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 31 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "data = pd.read_csv('creditcard.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(284807, 31)"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "        scaled_amount  scaled_time        V1        V2        V3        V4  \\\n69188       -0.181793    -0.368719 -1.373669  1.681458  0.856925  0.084847   \n183984       0.391253     0.485767  2.219304 -1.714562 -2.396642 -2.571874   \n170959       0.171872     0.419836  2.010373  0.000753 -1.952661  1.242297   \n150315       0.321246     0.097922  1.978214  0.038608 -1.758110  0.455132   \n225015       1.760637     0.697494 -1.472941  0.252156  3.014367  1.315021   \n\n              V5        V6        V7        V8  ...       V20       V21  \\\n69188  -0.905125 -1.214641 -0.008443  0.775643  ...  0.004643 -0.150017   \n183984  1.136436  3.443811 -1.582553  0.790591  ... -0.292766 -0.148788   \n170959  0.655689 -0.580965  0.491131 -0.206686  ... -0.324693  0.017948   \n150315  0.650015 -0.286199  0.007624 -0.130452  ... -0.117415 -0.467669   \n225015 -1.387980  1.807696 -0.734495 -0.249410  ... -0.476991  0.884970   \n\n             V22       V23       V24       V25       V26       V27       V28  \\\n69188  -0.583534  0.131279  0.688137 -0.139860  0.059670  0.124571  0.050170   \n183984 -0.091763  0.237288  0.688691 -0.141106 -0.144035  0.022049 -0.051637   \n170959  0.096671 -0.006121  0.369656  0.448490 -0.500643 -0.036769 -0.057618   \n150315 -1.149514  0.259380  0.028174 -0.291914  0.136220 -0.104944 -0.044384   \n225015  0.652405 -0.674504  0.057619  0.364067 -0.200854  0.118688  0.046721   \n\n        Class  \n69188       0  \n183984      0  \n170959      0  \n150315      0  \n225015      0  \n\n[5 rows x 31 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>scaled_amount</th>\n      <th>scaled_time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>...</th>\n      <th>V20</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>69188</th>\n      <td>-0.181793</td>\n      <td>-0.368719</td>\n      <td>-1.373669</td>\n      <td>1.681458</td>\n      <td>0.856925</td>\n      <td>0.084847</td>\n      <td>-0.905125</td>\n      <td>-1.214641</td>\n      <td>-0.008443</td>\n      <td>0.775643</td>\n      <td>...</td>\n      <td>0.004643</td>\n      <td>-0.150017</td>\n      <td>-0.583534</td>\n      <td>0.131279</td>\n      <td>0.688137</td>\n      <td>-0.139860</td>\n      <td>0.059670</td>\n      <td>0.124571</td>\n      <td>0.050170</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>183984</th>\n      <td>0.391253</td>\n      <td>0.485767</td>\n      <td>2.219304</td>\n      <td>-1.714562</td>\n      <td>-2.396642</td>\n      <td>-2.571874</td>\n      <td>1.136436</td>\n      <td>3.443811</td>\n      <td>-1.582553</td>\n      <td>0.790591</td>\n      <td>...</td>\n      <td>-0.292766</td>\n      <td>-0.148788</td>\n      <td>-0.091763</td>\n      <td>0.237288</td>\n      <td>0.688691</td>\n      <td>-0.141106</td>\n      <td>-0.144035</td>\n      <td>0.022049</td>\n      <td>-0.051637</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>170959</th>\n      <td>0.171872</td>\n      <td>0.419836</td>\n      <td>2.010373</td>\n      <td>0.000753</td>\n      <td>-1.952661</td>\n      <td>1.242297</td>\n      <td>0.655689</td>\n      <td>-0.580965</td>\n      <td>0.491131</td>\n      <td>-0.206686</td>\n      <td>...</td>\n      <td>-0.324693</td>\n      <td>0.017948</td>\n      <td>0.096671</td>\n      <td>-0.006121</td>\n      <td>0.369656</td>\n      <td>0.448490</td>\n      <td>-0.500643</td>\n      <td>-0.036769</td>\n      <td>-0.057618</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>150315</th>\n      <td>0.321246</td>\n      <td>0.097922</td>\n      <td>1.978214</td>\n      <td>0.038608</td>\n      <td>-1.758110</td>\n      <td>0.455132</td>\n      <td>0.650015</td>\n      <td>-0.286199</td>\n      <td>0.007624</td>\n      <td>-0.130452</td>\n      <td>...</td>\n      <td>-0.117415</td>\n      <td>-0.467669</td>\n      <td>-1.149514</td>\n      <td>0.259380</td>\n      <td>0.028174</td>\n      <td>-0.291914</td>\n      <td>0.136220</td>\n      <td>-0.104944</td>\n      <td>-0.044384</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>225015</th>\n      <td>1.760637</td>\n      <td>0.697494</td>\n      <td>-1.472941</td>\n      <td>0.252156</td>\n      <td>3.014367</td>\n      <td>1.315021</td>\n      <td>-1.387980</td>\n      <td>1.807696</td>\n      <td>-0.734495</td>\n      <td>-0.249410</td>\n      <td>...</td>\n      <td>-0.476991</td>\n      <td>0.884970</td>\n      <td>0.652405</td>\n      <td>-0.674504</td>\n      <td>0.057619</td>\n      <td>0.364067</td>\n      <td>-0.200854</td>\n      <td>0.118688</td>\n      <td>0.046721</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 31 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "std_scaler = StandardScaler()\n",
    "rob_scaler = RobustScaler()\n",
    "\n",
    "data['scaled_amount'] = rob_scaler.fit_transform(data['Amount'].values.reshape(-1,1))\n",
    "data['scaled_time'] = rob_scaler.fit_transform(data['Time'].values.reshape(-1,1))\n",
    "data.drop(['Time','Amount'], axis=1, inplace=True)\n",
    "\n",
    "scaled_amount = data['scaled_amount']\n",
    "scaled_time = data['scaled_time']\n",
    "\n",
    "data.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\n",
    "data.insert(0, 'scaled_amount', scaled_amount)\n",
    "data.insert(1, 'scaled_time', scaled_time)\n",
    "\n",
    "data.head()\n",
    "\n",
    "data = data.sample(frac=1)\n",
    "\n",
    "# amount of fraud classes 492 rows.\n",
    "fraud_data = data.loc[data['Class'] == 1]\n",
    "non_fraud_data = data.loc[data['Class'] == 0]\n",
    "\n",
    "normal_distributed_data = pd.concat([fraud_data, non_fraud_data])\n",
    "\n",
    "# Shuffle dataframe rows\n",
    "new_data = normal_distributed_data.sample(frac=1, random_state=42)\n",
    "\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using SMOTE\n",
    "\n",
    "# dataSmote=np.array(data.drop(\"Class\",axis=1))\n",
    "# y=np.array(data[['Class']])\n",
    "# smt=SMOTE()\n",
    "# dataSmote,y=smt.fit_sample(dataSmote,y)\n",
    "# dataSmote=pd.DataFrame(dataSmote)\n",
    "# y=pd.DataFrame(y)\n",
    "# dataSmote['Class']=y\n",
    "# dataSmote=dataSmote.sample(frac=1)\n",
    "# dataSmote.columns=list(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f, axes = plt.subplots(ncols=2, figsize=(20,6))\n",
    "\n",
    "# sb.countplot('Class',data=data,ax=axes[0])\n",
    "# axes[0].set_title('Fraud Distribution Original')\n",
    "\n",
    "# sb.countplot('Class',data=dataSmote,ax=axes[1])\n",
    "# axes[1].set_title('Fraud Distribution SMOTE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results={}\n",
    "aggregator=Aggregator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "datum=Data(data)\n",
    "\n",
    "Data_Global=datum.sample_smote(0,0.1)              #use datum.sample  if smote not required\n",
    "Data_Model_1A=datum.sample_smote(0.1,0.3)\n",
    "Data_Model_2A=datum.sample_smote(0.3,0.45)\n",
    "Data_Model_3A=datum.sample_smote(0.45,0.50)\n",
    "Data_Model_1B=datum.sample_smote(0.50,0.70)\n",
    "Data_Model_2B=datum.sample_smote(0.70,0.85)\n",
    "Data_Model_3B=datum.sample_smote(0.85,0.90)\n",
    "Data_Test=datum.sample_smote(0.90,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWeights(weights):\n",
    "    A=[]\n",
    "    for i in weights:\n",
    "        A.extend(np.array(i).flatten())\n",
    "    B=[]\n",
    "    for i in A:\n",
    "        B.extend(i.flatten())\n",
    "\n",
    "    \n",
    "    return np.array(B)\n",
    "\n",
    "def getAdversarialData(probabilities,data,gradient,result):\n",
    "    output=[]\n",
    "    for index,i in enumerate(probabilities.reshape(-1)):\n",
    "        temp=[]\n",
    "        temp.append(i)\n",
    "        temp.extend(np.array(data[index:index+1])[0])\n",
    "        temp.extend(gradient)\n",
    "        temp.append(result)\n",
    "        output.append(temp)\n",
    "    return np.array(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From C:\\Users\\nomif\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n\nWARNING:tensorflow:From C:\\Users\\nomif\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n\nWARNING:tensorflow:From C:\\Users\\nomif\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n\nWARNING:tensorflow:From C:\\Users\\nomif\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n\nWARNING:tensorflow:From C:\\Users\\nomif\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n\nWARNING:tensorflow:From C:\\Users\\nomif\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\nWARNING:tensorflow:From C:\\Users\\nomif\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n\nEpoch 1/5\n45476/45476 [==============================] - 2s 49us/step - loss: 0.0453 - acc: 0.9874\nEpoch 2/5\n45476/45476 [==============================] - 1s 29us/step - loss: 0.0034 - acc: 0.9995\nEpoch 3/5\n45476/45476 [==============================] - 2s 35us/step - loss: 0.0025 - acc: 0.9996\nEpoch 4/5\n45476/45476 [==============================] - 1s 30us/step - loss: 0.0020 - acc: 0.9997\nEpoch 5/5\n45476/45476 [==============================] - 1s 33us/step - loss: 0.0013 - acc: 0.9998\n11370/11370 [==============================] - 0s 15us/step\nSaved model to disk\n"
    }
   ],
   "source": [
    "GlobalBank=Bank(Data_Global,0.2)\n",
    "GlobalBank.run(GlobalBank.X_train, GlobalBank.Y_train,load=False)\n",
    "\n",
    "results['BankG.1']=GlobalBank.evaluate(GlobalBank.X_test,GlobalBank.Y_test)\n",
    "\n",
    "GlobalBank.saveModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Loaded model from disk\nEpoch 1/5\n30/30 [==============================] - 1s 29ms/step - loss: 0.9112 - acc: 0.9333\nEpoch 2/5\n30/30 [==============================] - 0s 33us/step - loss: 0.8874 - acc: 0.9333\nEpoch 3/5\n30/30 [==============================] - 0s 67us/step - loss: 0.8729 - acc: 0.9333\nEpoch 4/5\n30/30 [==============================] - 0s 33us/step - loss: 0.8597 - acc: 0.9333\nEpoch 5/5\n30/30 [==============================] - 0s 66us/step - loss: 0.8467 - acc: 0.9333\nLoaded model from disk\nEpoch 1/5\n30/30 [==============================] - 1s 28ms/step - loss: 2.7481 - acc: 0.8000\nEpoch 2/5\n30/30 [==============================] - 0s 33us/step - loss: 2.7212 - acc: 0.8000\nEpoch 3/5\n30/30 [==============================] - 0s 33us/step - loss: 2.7029 - acc: 0.8333\nEpoch 4/5\n30/30 [==============================] - 0s 100us/step - loss: 2.6934 - acc: 0.8333\nEpoch 5/5\n30/30 [==============================] - 0s 66us/step - loss: 2.6894 - acc: 0.8333\nLoaded model from disk\nEpoch 1/5\n30/30 [==============================] - 1s 34ms/step - loss: 1.2569 - acc: 0.9000\nEpoch 2/5\n30/30 [==============================] - 0s 32us/step - loss: 1.2041 - acc: 0.9000\nEpoch 3/5\n30/30 [==============================] - 0s 100us/step - loss: 1.1488 - acc: 0.9000\nEpoch 4/5\n30/30 [==============================] - 0s 66us/step - loss: 1.1054 - acc: 0.9000\nEpoch 5/5\n30/30 [==============================] - 0s 66us/step - loss: 1.0844 - acc: 0.9333\nLoaded model from disk\nEpoch 1/5\n30/30 [==============================] - 1s 35ms/step - loss: 2.0408 - acc: 0.8333\nEpoch 2/5\n30/30 [==============================] - 0s 67us/step - loss: 1.9625 - acc: 0.8333\nEpoch 3/5\n30/30 [==============================] - 0s 66us/step - loss: 1.8828 - acc: 0.8333\nEpoch 4/5\n30/30 [==============================] - 0s 67us/step - loss: 1.7972 - acc: 0.8667\nEpoch 5/5\n30/30 [==============================] - 0s 66us/step - loss: 1.7268 - acc: 0.8667\nLoaded model from disk\nEpoch 1/5\n30/30 [==============================] - 1s 37ms/step - loss: 0.5468 - acc: 0.9667\nEpoch 2/5\n30/30 [==============================] - 0s 57us/step - loss: 0.5414 - acc: 0.9667\nEpoch 3/5\n30/30 [==============================] - 0s 67us/step - loss: 0.5391 - acc: 0.9667\nEpoch 4/5\n30/30 [==============================] - 0s 66us/step - loss: 0.5381 - acc: 0.9667\nEpoch 5/5\n30/30 [==============================] - 0s 100us/step - loss: 0.5377 - acc: 0.9667\nLoaded model from disk\nEpoch 1/5\n30/30 [==============================] - 1s 36ms/step - loss: 2.2147 - acc: 0.8000\nEpoch 2/5\n30/30 [==============================] - 0s 65us/step - loss: 2.1400 - acc: 0.8333\nEpoch 3/5\n30/30 [==============================] - 0s 66us/step - loss: 2.1036 - acc: 0.8333\nEpoch 4/5\n30/30 [==============================] - 0s 67us/step - loss: 2.0414 - acc: 0.8333\nEpoch 5/5\n30/30 [==============================] - 0s 200us/step - loss: 1.9904 - acc: 0.8333\nLoaded model from disk\nEpoch 1/5\n30/30 [==============================] - 1s 39ms/step - loss: 1.5169 - acc: 0.8000\nEpoch 2/5\n30/30 [==============================] - 0s 33us/step - loss: 1.3222 - acc: 0.8333\nEpoch 3/5\n30/30 [==============================] - 0s 67us/step - loss: 1.1838 - acc: 0.8667\nEpoch 4/5\n30/30 [==============================] - 0s 100us/step - loss: 1.1326 - acc: 0.9000\nEpoch 5/5\n30/30 [==============================] - 0s 99us/step - loss: 1.1096 - acc: 0.9000\nLoaded model from disk\nEpoch 1/5\n30/30 [==============================] - 1s 43ms/step - loss: 1.0471 - acc: 0.8333\nEpoch 2/5\n30/30 [==============================] - 0s 98us/step - loss: 0.9564 - acc: 0.8667\nEpoch 3/5\n30/30 [==============================] - 0s 67us/step - loss: 0.8845 - acc: 0.8667\nEpoch 4/5\n30/30 [==============================] - 0s 66us/step - loss: 0.8192 - acc: 0.9000\nEpoch 5/5\n30/30 [==============================] - 0s 66us/step - loss: 0.7612 - acc: 0.9000\nLoaded model from disk\nEpoch 1/5\n30/30 [==============================] - 1s 42ms/step - loss: 0.7543 - acc: 0.9000\nEpoch 2/5\n30/30 [==============================] - 0s 66us/step - loss: 0.7157 - acc: 0.9333\nEpoch 3/5\n30/30 [==============================] - 0s 100us/step - loss: 0.6750 - acc: 0.9333\nEpoch 4/5\n30/30 [==============================] - 0s 66us/step - loss: 0.6429 - acc: 0.9333\nEpoch 5/5\n30/30 [==============================] - 0s 66us/step - loss: 0.6161 - acc: 0.9333\nLoaded model from disk\nEpoch 1/5\n30/30 [==============================] - 1s 45ms/step - loss: 1.0746 - acc: 0.9333\nEpoch 2/5\n30/30 [==============================] - 0s 33us/step - loss: 1.0746 - acc: 0.9333\nEpoch 3/5\n30/30 [==============================] - 0s 100us/step - loss: 1.0745 - acc: 0.9333\nEpoch 4/5\n30/30 [==============================] - 0s 100us/step - loss: 1.0745 - acc: 0.9333\nEpoch 5/5\n30/30 [==============================] - 0s 66us/step - loss: 1.0745 - acc: 0.9333\n"
    }
   ],
   "source": [
    "result=[]\n",
    "for i in range(10):\n",
    "    Bank1=Bank(Data_Model_1A[i*30:(i+1)*30])\n",
    "    Bank1.run(Bank1.X_train,Bank1.Y_train)\n",
    "    gradient=getWeights(GlobalBank.getWeights()) - getWeights(Bank1.getWeights())\n",
    "    result.extend(\n",
    "        getAdversarialData(Bank1.predict(Bank1.X_test),Bank1.X_test,gradient,True)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(300, 1697)"
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "np.array(result).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(60, 1697)"
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "temp=[]\n",
    "temp.extend(output)\n",
    "temp.extend(output)\n",
    "np.array(temp).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "Train on 81882 samples, validate on 9098 samples\n",
      "Epoch 1/5\n",
      "81882/81882 [==============================] - 12s 147us/step - loss: 0.0583 - acc: 0.9860 - val_loss: 0.0424 - val_acc: 0.9923\n",
      "Epoch 2/5\n",
      "81882/81882 [==============================] - 11s 130us/step - loss: 0.0575 - acc: 0.9872 - val_loss: 0.0454 - val_acc: 0.9918\n",
      "Epoch 3/5\n",
      "81882/81882 [==============================] - 13s 156us/step - loss: 0.0541 - acc: 0.9881 - val_loss: 0.0489 - val_acc: 0.9900\n",
      "Epoch 4/5\n",
      "81882/81882 [==============================] - 11s 138us/step - loss: 0.0551 - acc: 0.9878 - val_loss: 0.0385 - val_acc: 0.9951\n",
      "Epoch 5/5\n",
      "81882/81882 [==============================] - 12s 148us/step - loss: 0.0525 - acc: 0.9887 - val_loss: 0.0401 - val_acc: 0.9923\n",
      "Loaded model from disk\n",
      "Train on 61411 samples, validate on 6824 samples\n",
      "Epoch 1/5\n",
      "61411/61411 [==============================] - 10s 165us/step - loss: 0.0590 - acc: 0.9856 - val_loss: 0.0418 - val_acc: 0.9930\n",
      "Epoch 2/5\n",
      "61411/61411 [==============================] - 9s 146us/step - loss: 0.0573 - acc: 0.9868 - val_loss: 0.0431 - val_acc: 0.9914\n",
      "Epoch 3/5\n",
      "61411/61411 [==============================] - 10s 159us/step - loss: 0.0560 - acc: 0.9872 - val_loss: 0.0431 - val_acc: 0.9921\n",
      "Epoch 4/5\n",
      "61411/61411 [==============================] - 9s 153us/step - loss: 0.0554 - acc: 0.9875 - val_loss: 0.0423 - val_acc: 0.9916\n",
      "Epoch 5/5\n",
      "61411/61411 [==============================] - 9s 148us/step - loss: 0.0541 - acc: 0.9879 - val_loss: 0.0433 - val_acc: 0.9925\n",
      "Loaded model from disk\n",
      "Train on 225177 samples, validate on 25020 samples\n",
      "Epoch 1/5\n",
      "225177/225177 [==============================] - 34s 152us/step - loss: 0.0566 - acc: 0.9869 - val_loss: 0.0443 - val_acc: 0.9930\n",
      "Epoch 2/5\n",
      "225177/225177 [==============================] - 34s 149us/step - loss: 0.0528 - acc: 0.9888 - val_loss: 0.0399 - val_acc: 0.9933\n",
      "Epoch 3/5\n",
      "225177/225177 [==============================] - 33s 144us/step - loss: 0.0522 - acc: 0.9889 - val_loss: 0.0393 - val_acc: 0.9933\n",
      "Epoch 4/5\n",
      "225177/225177 [==============================] - 33s 149us/step - loss: 0.0510 - acc: 0.9895 - val_loss: 0.0372 - val_acc: 0.9944\n",
      "Epoch 5/5\n",
      "225177/225177 [==============================] - 35s 154us/step - loss: 0.0507 - acc: 0.9898 - val_loss: 0.0473 - val_acc: 0.9905\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "Bank1=Bank(Data_Model_1A,0.2)\n",
    "Bank1.run(Bank1.X_train,Bank1.Y_train)\n",
    "\n",
    "Bank2=Bank(Data_Model_2A,0.2)\n",
    "Bank2.run(Bank2.X_train,Bank2.Y_train)\n",
    "\n",
    "Bank3=Bank(Data_Model_3A,0.2)\n",
    "Bank3.run(Bank3.X_train,Bank3.Y_train)\n",
    "\n",
    "delta=aggregator.aggregate(GlobalBank.getWeights(),Bank1.getWeights(),Bank2.getWeights(),Bank3.getWeights())\n",
    "\n",
    "GlobalBank.setWeights(delta)\n",
    "GlobalBank.saveModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22746/22746 [==============================] - 1s 55us/step\n",
      "17059/17059 [==============================] - 1s 48us/step\n",
      "62550/62550 [==============================] - 3s 49us/step\n",
      "11373/11373 [==============================] - 1s 56us/step\n"
     ]
    }
   ],
   "source": [
    "results['Bank1.1']=Bank1.evaluate(Bank1.X_test,Bank1.Y_test)\n",
    "results['Bank2.1']=Bank2.evaluate(Bank2.X_test,Bank2.Y_test)\n",
    "results['Bank3.1']=Bank3.evaluate(Bank3.X_test,Bank3.Y_test)\n",
    "results['BankG.2']=GlobalBank.evaluate(GlobalBank.X_test,GlobalBank.Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "Train on 204706 samples, validate on 22746 samples\n",
      "Epoch 1/5\n",
      "204706/204706 [==============================] - 36s 178us/step - loss: 0.0524 - acc: 0.9883 - val_loss: 0.0327 - val_acc: 0.9956\n",
      "Epoch 2/5\n",
      "204706/204706 [==============================] - 33s 159us/step - loss: 0.0529 - acc: 0.9888 - val_loss: 0.0373 - val_acc: 0.9940\n",
      "Epoch 3/5\n",
      "204706/204706 [==============================] - 32s 154us/step - loss: 0.0525 - acc: 0.9892 - val_loss: 0.0380 - val_acc: 0.9942\n",
      "Epoch 4/5\n",
      "204706/204706 [==============================] - 31s 152us/step - loss: 0.0521 - acc: 0.9893 - val_loss: 0.0384 - val_acc: 0.9941\n",
      "Epoch 5/5\n",
      "204706/204706 [==============================] - 31s 151us/step - loss: 0.0516 - acc: 0.9895 - val_loss: 0.0381 - val_acc: 0.9940\n",
      "Loaded model from disk\n",
      "Train on 122823 samples, validate on 13648 samples\n",
      "Epoch 1/5\n",
      "122823/122823 [==============================] - 19s 157us/step - loss: 0.0529 - acc: 0.9879 - val_loss: 0.0365 - val_acc: 0.9943\n",
      "Epoch 2/5\n",
      "122823/122823 [==============================] - 21s 167us/step - loss: 0.0520 - acc: 0.9886 - val_loss: 0.0350 - val_acc: 0.9945\n",
      "Epoch 3/5\n",
      "122823/122823 [==============================] - 19s 153us/step - loss: 0.0524 - acc: 0.9891 - val_loss: 0.0370 - val_acc: 0.9941\n",
      "Epoch 4/5\n",
      "122823/122823 [==============================] - 19s 151us/step - loss: 0.0513 - acc: 0.9893 - val_loss: 0.0397 - val_acc: 0.9933\n",
      "Epoch 5/5\n",
      "122823/122823 [==============================] - 19s 152us/step - loss: 0.0520 - acc: 0.9891 - val_loss: 0.0432 - val_acc: 0.9910\n",
      "Loaded model from disk\n",
      "Train on 61412 samples, validate on 6824 samples\n",
      "Epoch 1/5\n",
      "61412/61412 [==============================] - 11s 175us/step - loss: 0.0532 - acc: 0.9871 - val_loss: 0.0377 - val_acc: 0.9943\n",
      "Epoch 2/5\n",
      "61412/61412 [==============================] - 9s 153us/step - loss: 0.0514 - acc: 0.9884 - val_loss: 0.0364 - val_acc: 0.9937\n",
      "Epoch 3/5\n",
      "61412/61412 [==============================] - 10s 164us/step - loss: 0.0509 - acc: 0.9893 - val_loss: 0.0512 - val_acc: 0.9867\n",
      "Epoch 4/5\n",
      "61412/61412 [==============================] - 10s 163us/step - loss: 0.0508 - acc: 0.9890 - val_loss: 0.0374 - val_acc: 0.9944\n",
      "Epoch 5/5\n",
      "61412/61412 [==============================] - 10s 163us/step - loss: 0.0488 - acc: 0.9898 - val_loss: 0.0393 - val_acc: 0.9933\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Bank1.setData(Data_Model_1B,0.2)\n",
    "Bank1.run(Bank1.X_train,Bank1.Y_train)\n",
    "\n",
    "Bank2.setData(Data_Model_2B,0.2)\n",
    "Bank2.run(Bank2.X_train,Bank2.Y_train)\n",
    "\n",
    "Bank3.setData(Data_Model_3B,0.2)\n",
    "Bank3.run(Bank3.X_train,Bank3.Y_train)\n",
    "\n",
    "\n",
    "\n",
    "delta=aggregator.aggregate(GlobalBank.getWeights(),Bank1.getWeights(),Bank2.getWeights(),Bank3.getWeights())\n",
    "\n",
    "GlobalBank.setWeights(delta)\n",
    "GlobalBank.saveModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56863/56863 [==============================] - 3s 56us/step\n",
      "34118/34118 [==============================] - 2s 59us/step\n",
      "17059/17059 [==============================] - 1s 72us/step\n"
     ]
    }
   ],
   "source": [
    "results['Bank1.2']=Bank1.evaluate(Bank1.X_test,Bank1.Y_test)\n",
    "results['Bank2.2']=Bank2.evaluate(Bank2.X_test,Bank2.Y_test)\n",
    "results['Bank3.2']=Bank3.evaluate(Bank3.X_test,Bank3.Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51177/51177 [==============================] - 3s 57us/step\n"
     ]
    }
   ],
   "source": [
    "GlobalBank.setData(Data_Test,0.9)\n",
    "results['BankG.3']=GlobalBank.evaluate(GlobalBank.X_test,GlobalBank.Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non Federated Learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}